{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEKArgm2uBViY8Ppg3HS2/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renatoerss/vch-srna-pred/blob/main/vch_srna_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYx5hpaM4mYQ",
        "outputId": "a7e820e5-47e4-485e-fbdb-371c677aaf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -q update\n",
        "!apt-get -q install -y bedtools\n",
        "!pip install -q pybigwig pandas numpy scipy pybedtools gffutils openpyxl\n",
        "\n",
        "import shutil, importlib\n",
        "for pip_name, mod in [(\"pybigwig\",\"pyBigWig\"),(\"pandas\",\"pandas\"),(\"numpy\",\"numpy\"),\n",
        "                      (\"scipy\",\"scipy\"),(\"pybedtools\",\"pybedtools\"),(\"gffutils\",\"gffutils\"),(\"openpyxl\",\"openpyxl\")]:\n",
        "    try:\n",
        "        m = importlib.import_module(mod)\n",
        "        print(\"OK\", pip_name, getattr(m,\"__version__\", \"\"))\n",
        "    except Exception as e:\n",
        "        print(\"ERRO\", pip_name, e)\n",
        "print(\"bedtools:\", shutil.which(\"bedtools\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tznV1AGx6uu-",
        "outputId": "18290300-d616-437f-bd0a-defc1459b760"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,425 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,275 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,727 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,811 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,336 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,582 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,750 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,922 kB]\n",
            "Fetched 34.3 MB in 4s (9,530 kB/s)\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  bedtools\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 563 kB of archives.\n",
            "After this operation, 1,548 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 bedtools amd64 2.30.0+dfsg-2ubuntu0.1 [563 kB]\n",
            "Fetched 563 kB in 1s (763 kB/s)\n",
            "Selecting previously unselected package bedtools.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../bedtools_2.30.0+dfsg-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking bedtools (2.30.0+dfsg-2ubuntu0.1) ...\n",
            "Setting up bedtools (2.30.0+dfsg-2ubuntu0.1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.1/187.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pybedtools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "OK pybigwig 0.3.24\n",
            "OK pandas 2.2.2\n",
            "OK numpy 2.0.2\n",
            "OK scipy 1.16.2\n",
            "OK pybedtools 0.12.0\n",
            "OK gffutils 0.13\n",
            "OK openpyxl 3.1.5\n",
            "bedtools: /usr/bin/bedtools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuration and Input Files\n",
        "\n",
        "This section defines all input files and adjustable parameters for the analysis.  \n",
        "Update the file paths (`/path/to/...`) to match your own dataset locations.\n",
        "\n",
        "**Required inputs**\n",
        "- Four strand-specific RNA-seq bigWig files:\n",
        "  - `CRP (+/−)` and `K52Q (+/−)`\n",
        "- Two genome annotations (GFF3 or converted XLSX), one per chromosome  \n",
        "  *(e.g., `NC_002505` and `NC_002506`)*\n",
        "- Two TSS maps in BED-like format (`TSS_plus.txt`, `TSS_minus.txt`)\n",
        "\n",
        "**Output structure**\n",
        "All results will be written automatically to the directory specified by `OUT_DIR`,  \n",
        "with the following subfolders created for organization:\n",
        "\n"
      ],
      "metadata": {
        "id": "r3PvZbUGJ3o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Adjustable parameters**\n",
        "- `SMOOTH_WIN` – smoothing window size for bigWig traces  \n",
        "- `MIN_WIDTH_BP`, `MAX_WIDTH_BP` – min/max allowed peak width (bp)  \n",
        "- `MIN_PROM` – minimum prominence threshold  \n",
        "- `MIN_AREA` – minimum integrated area threshold  \n",
        "- `FOLD_MIN` – minimum fold-difference between K52Q and CRP  \n",
        "- `PROFILE_STEP` – sampling interval for quick QC profiling  \n",
        "\n",
        "Before running the analysis:\n",
        "1. Confirm that all paths exist by running the `_check()` helper.  \n",
        "2. Adjust thresholds based on signal scale (median, p95/p99) from the QC step.  \n",
        "3. Keep all edits confined to this configuration cell for reproducibility.\n"
      ],
      "metadata": {
        "id": "WuZOVswYJ3m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 1) CONFIGURATION — EDIT THIS SECTION ONLY\n",
        "# ===============================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import os, json, re\n",
        "\n",
        "# ====== INPUT FILES (EDIT THESE PATHS) ======\n",
        "\n",
        "# bigWig tracks for strand-specific RNA-seq data\n",
        "BIGWIGS = {\n",
        "    \"CRP\": {\n",
        "        \"+\": \"/path/to/CRP_plus_stranded.bw\",   # e.g., Drive or local path\n",
        "        \"-\": \"/path/to/CRP_minus_stranded.bw\",\n",
        "    },\n",
        "    \"K52Q\": {\n",
        "        \"+\": \"/path/to/K52Q_plus_stranded.bw\",\n",
        "        \"-\": \"/path/to/K52Q_minus_stranded.bw\",\n",
        "    }\n",
        "}\n",
        "\n",
        "# GFF3 or converted XLSX genome annotation files\n",
        "# one per chromosome (example for Vibrio cholerae)\n",
        "GFF3 = {\"GFF3\": {\n",
        "    \"NC_002505\": \"/path/to/NC_002505_GFF3converted.xlsx\",\n",
        "    \"NC_002506\": \"/path/to/NC_002506_GFF3converted.xlsx\",\n",
        "}}\n",
        "\n",
        "# TSS maps (BED-like format; strand-aware)\n",
        "TSS_PLUS  = \"/path/to/TSS_plus.txt\"\n",
        "TSS_MINUS = \"/path/to/TSS_minus.txt\"\n",
        "\n",
        "# Output directory (all results will be stored here)\n",
        "OUT_DIR = \"/path/to/output/srna_scan_YYYYMMDD\"\n",
        "\n",
        "# ====== ANALYSIS PARAMETERS (ADJUST AS NEEDED) ======\n",
        "\n",
        "SMOOTH_WIN   = 31            # smoothing window size (odd integer)\n",
        "MIN_WIDTH_BP = 50            # minimum peak width (bp)\n",
        "MAX_WIDTH_BP = 350           # maximum peak width (bp)\n",
        "MIN_PROM     = 2e4           # minimum peak prominence (adjust per p95/p99)\n",
        "MIN_AREA     = 3e5           # minimum integrated area of the peak\n",
        "FOLD_MIN     = 5.0           # minimum fold-change (K52Q vs CRP)\n",
        "PROFILE_STEP = 10            # sampling step for QC profiling\n",
        "# ===============================================================\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Quick existence check utility\n",
        "# ---------------------------------------------------------------\n",
        "def _check(p):\n",
        "    \"\"\"Check if a file or folder exists and print result.\"\"\"\n",
        "    print(Path(p).exists(), p)\n",
        "\n",
        "# Example usage (uncomment lines below to verify file paths)\n",
        "# for k in (\"+\",\"-\"):\n",
        "#     _check(BIGWIGS[\"CRP\"][k]); _check(BIGWIGS[\"K52Q\"][k])\n",
        "# _check(GFF3[\"GFF3\"][\"NC_002505\"]); _check(GFF3[\"GFF3\"][\"NC_002506\"])\n",
        "# _check(TSS_PLUS); _check(TSS_MINUS)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Prepare output directories\n",
        "# ---------------------------------------------------------------\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "(Path(OUT_DIR) / \"mask\").mkdir(exist_ok=True)\n",
        "(Path(OUT_DIR) / \"qc\").mkdir(exist_ok=True)\n",
        "(Path(OUT_DIR) / \"peaks\").mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "W5ibt25iJmbe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, csv, warnings, itertools\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyBigWig\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ---------------- Utilities ----------------\n",
        "def canonical(chrom: str) -> str:\n",
        "    \"\"\"Remove version suffixes (e.g., '.1') for normalization.\"\"\"\n",
        "    return chrom.split('.')[0]\n",
        "\n",
        "def read_bigwig(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"BigWig does not exist: {path}\")\n",
        "    return pyBigWig.open(path)\n",
        "\n",
        "def chrom_sizes_from_bw(bw):\n",
        "    \"\"\"Return dict {canonical: (real_name, length)}.\"\"\"\n",
        "    out = {}\n",
        "    for name, length in bw.chroms().items():\n",
        "        can = canonical(name)\n",
        "        if can not in out:\n",
        "            out[can] = (name, length)\n",
        "        else:\n",
        "            # size consistency check\n",
        "            if out[can][1] != length:\n",
        "                print(f\"[WARNING] Different sizes for {can}: {out[can][1]} vs {length}\")\n",
        "    return out\n",
        "\n",
        "def norm_chrom_name_for_bw(bw, can_name):\n",
        "    \"\"\"Map canonical name to the actual chromosome name used in the BigWig.\"\"\"\n",
        "    for n, l in bw.chroms().items():\n",
        "        if canonical(n) == can_name:\n",
        "            return n\n",
        "    return None\n",
        "\n",
        "def read_gff_any(path):\n",
        "    \"\"\"\n",
        "    Read GFF3 (.gff, .gff3, .gff.gz) OR converted XLSX.\n",
        "    Returns DataFrame with columns: chrom, type, start, end, strand\n",
        "    \"\"\"\n",
        "    path = str(path)\n",
        "    if path.endswith((\".gff\", \".gff3\", \".gff.gz\")):\n",
        "        rows = []\n",
        "        opener = open\n",
        "        if path.endswith(\".gz\"):\n",
        "            import gzip\n",
        "            opener = gzip.open\n",
        "        with opener(path, \"rt\") as fh:\n",
        "            for ln in fh:\n",
        "                if not ln or ln.startswith(\"#\"):\n",
        "                    continue\n",
        "                parts = ln.rstrip(\"\\n\").split(\"\\t\")\n",
        "                if len(parts) < 5:\n",
        "                    continue\n",
        "                seqid, source, typ, start, end = parts[0], parts[1], parts[2], int(parts[3]), int(parts[4])\n",
        "                strand = parts[6] if len(parts) > 6 else \".\"\n",
        "                rows.append((canonical(seqid), typ, start, end, strand))\n",
        "        df = pd.DataFrame(rows, columns=[\"chrom\",\"type\",\"start\",\"end\",\"strand\"])\n",
        "        return df\n",
        "    elif path.endswith((\".xlsx\",\".xls\")):\n",
        "        x = pd.read_excel(path, engine=\"openpyxl\")\n",
        "        # normalize column names\n",
        "        cols = {c.lower(): c for c in x.columns}\n",
        "        # try synonyms\n",
        "        chrom_col = next((cols[k] for k in [\"seqid\",\"chrom\",\"chromosome\",\"contig\",\"chr\"] if k in cols), None)\n",
        "        type_col  = next((cols[k] for k in [\"type\",\"feature\",\"feat_type\"] if k in cols), None)\n",
        "        start_col = next((cols[k] for k in [\"start\",\"start_bp\",\"start_pos\",\"begin\"] if k in cols), None)\n",
        "        end_col   = next((cols[k] for k in [\"end\",\"end_bp\",\"end_pos\",\"stop\"] if k in cols), None)\n",
        "        strand_col= next((cols[k] for k in [\"strand\",\"str\"] if k in cols), None)\n",
        "\n",
        "        if not all([chrom_col, type_col, start_col, end_col]):\n",
        "            raise ValueError(f\"XLSX is missing required columns (chrom/type/start/end): {path}\")\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            \"chrom\":  x[chrom_col].astype(str).map(canonical),\n",
        "            \"type\":   x[type_col].astype(str),\n",
        "            \"start\":  x[start_col].astype(int),\n",
        "            \"end\":    x[end_col].astype(int),\n",
        "            \"strand\": x[strand_col].astype(str) if strand_col else \".\",\n",
        "        })\n",
        "        return df\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported GFF format: {path}\")\n",
        "\n",
        "def merge_intervals(intervals):\n",
        "    \"\"\"intervals: list[(start,end)] -> merged, sorted, non-overlapping intervals.\"\"\"\n",
        "    if not intervals:\n",
        "        return []\n",
        "    arr = sorted(intervals, key=lambda x: (x[0], x[1]))\n",
        "    merged = [arr[0]]\n",
        "    for s,e in arr[1:]:\n",
        "        ls, le = merged[-1]\n",
        "        if s <= le + 1:\n",
        "            merged[-1] = (ls, max(le, e))\n",
        "        else:\n",
        "            merged.append((s,e))\n",
        "    return merged\n",
        "\n",
        "def complement_intervals(covered, chrom_len):\n",
        "    \"\"\"covered intervals [(s,e)] -> complement within [1, chrom_len] (1-based).\"\"\"\n",
        "    inter = []\n",
        "    prev_end = 0\n",
        "    for s,e in covered:\n",
        "        if s > prev_end + 1:\n",
        "            inter.append((prev_end+1, s-1))\n",
        "        prev_end = e\n",
        "    if prev_end < chrom_len:\n",
        "        inter.append((prev_end+1, chrom_len))\n",
        "    return inter\n",
        "\n",
        "def write_bed(path, chrom, intervals):\n",
        "    with open(path, \"a\") as fh:\n",
        "        for s,e in intervals:\n",
        "            fh.write(f\"{chrom}\\t{s-1}\\t{e}\\n\")  # BED 0-based half-open\n",
        "\n",
        "def values_from_bw(bw, chrom_real, start_1, end_1):\n",
        "    \"\"\"Return np.array of 1-bp values; replace NaN with 0.\"\"\"\n",
        "    # BigWig uses 0-based; here we use 1-based coordinates\n",
        "    arr = np.array(bw.values(chrom_real, start_1-1, end_1), dtype=float)\n",
        "    arr[np.isnan(arr)] = 0.0\n",
        "    return arr\n",
        "\n",
        "def moving_average(x, win):\n",
        "    if win <= 1:\n",
        "        return x\n",
        "    # enforce odd window\n",
        "    if win % 2 == 0:\n",
        "        win += 1\n",
        "    k = np.ones(win, dtype=float) / win\n",
        "    return np.convolve(x, k, mode=\"same\")\n",
        "\n",
        "# ------------- Basic QC -------------\n",
        "def qc_open_and_common_chroms(BIGWIGS):\n",
        "    bws = {cond: {st: read_bigwig(path) for st, path in d.items()} for cond, d in BIGWIGS.items()}\n",
        "    # collect sizes from one reference (CRP '+')\n",
        "    ref_bw = bws[\"CRP\"][\"+\"]\n",
        "    sizes_ref = chrom_sizes_from_bw(ref_bw)  # {can: (real, len)}\n",
        "    # find common chromosomes across all BigWigs\n",
        "    commons = set(sizes_ref.keys())\n",
        "    for cond, d in bws.items():\n",
        "        for st, bw in d.items():\n",
        "            sizes = chrom_sizes_from_bw(bw).keys()\n",
        "            commons = commons.intersection(set(sizes))\n",
        "    commons = sorted(list(commons))\n",
        "    print(\"[QC] Common chromosomes (canonical):\", commons)\n",
        "    return bws, commons, sizes_ref\n",
        "\n",
        "def read_mask_intergenic(GFF3_paths_by_can, commons, sizes_map):\n",
        "    # Read GFF and filter to CDS/rRNA/tRNA only\n",
        "    gff_all = []\n",
        "    for can, path in GFF3_paths_by_can.items():\n",
        "        if can not in commons:\n",
        "            print(f\"[NOTICE] {can} is not in the common set; skipping.\")\n",
        "            continue\n",
        "        df = read_gff_any(path)\n",
        "        df = df[df[\"chrom\"] == can].copy()\n",
        "        df = df[df[\"type\"].isin([\"CDS\",\"rRNA\",\"tRNA\"])]\n",
        "        gff_all.append(df)\n",
        "    if not gff_all:\n",
        "        raise RuntimeError(\"No valid GFF after filtering.\")\n",
        "    gff = pd.concat(gff_all, ignore_index=True)\n",
        "    # merge per chromosome\n",
        "    covered_by_chrom = {}\n",
        "    for can in sorted(set(gff[\"chrom\"])):\n",
        "        sub = gff[gff[\"chrom\"]==can][[\"start\",\"end\"]].to_numpy()\n",
        "        merged = merge_intervals([(int(s),int(e)) for s,e in sub])\n",
        "        covered_by_chrom[can] = merged\n",
        "    # intergenic complement\n",
        "    interg_by_chrom = {}\n",
        "    for can in covered_by_chrom:\n",
        "        chrom_len = sizes_map[can][1]\n",
        "        interg_by_chrom[can] = complement_intervals(covered_by_chrom[can], chrom_len)\n",
        "    return covered_by_chrom, interg_by_chrom\n",
        "\n",
        "def report_mask_intergenic(covered_by_chrom, interg_by_chrom, sizes_map):\n",
        "    rows = []\n",
        "    for can in sorted(interg_by_chrom.keys()):\n",
        "        cov_bp = sum(e - s + 1 for s,e in covered_by_chrom[can])\n",
        "        ig_bp  = sum(e - s + 1 for s,e in interg_by_chrom[can])\n",
        "        total  = sizes_map[can][1]\n",
        "        rows.append((can, total, cov_bp, ig_bp, 100*cov_bp/total, 100*ig_bp/total, len(interg_by_chrom[can])))\n",
        "    df = pd.DataFrame(rows, columns=[\"chrom\",\"len_bp\",\"masked_bp\",\"intergenic_bp\",\"masked_%\",\"intergenic_%\",\"n_intergenic\"])\n",
        "    return df\n",
        "\n",
        "# ------------- Signal profiling -------------\n",
        "def profile_track_over_intergenics(bw, sizes_map, interg_by_chrom, step=10, smooth_win=31):\n",
        "    vals = []\n",
        "    for can, intervals in interg_by_chrom.items():\n",
        "        chrom_real = sizes_map[can][0]\n",
        "        for s,e in intervals:\n",
        "            if e - s + 1 < 3:\n",
        "                continue\n",
        "            # sample every 'step' bp\n",
        "            xs = values_from_bw(bw, chrom_real, s, e)\n",
        "            if step > 1:\n",
        "                xs = xs[::step]\n",
        "            if smooth_win and smooth_win > 1:\n",
        "                xs = moving_average(xs, smooth_win)\n",
        "            vals.append(xs)\n",
        "    if not vals:\n",
        "        return {\"count_points\": 0}\n",
        "    v = np.concatenate(vals)\n",
        "    v = v[np.isfinite(v)]\n",
        "    if v.size == 0:\n",
        "        return {\"count_points\": 0}\n",
        "    med = float(np.median(v))\n",
        "    mad = float(np.median(np.abs(v - med)))\n",
        "    p95 = float(np.percentile(v, 95))\n",
        "    p99 = float(np.percentile(v, 99))\n",
        "    frac_pos = float(np.mean(v > 0))\n",
        "    return {\"count_points\": int(v.size), \"median\": med, \"MAD\": mad, \"p95\": p95, \"p99\": p99, \"frac_gt0\": frac_pos}\n",
        "\n",
        "# ------------- TSS (nearest) -------------\n",
        "def read_tss_file(path):\n",
        "    \"\"\"Read BED-like file (ignores 'track'). Return DF: chrom, pos(1-based), name, strand.\"\"\"\n",
        "    rows = []\n",
        "    with open(path, \"r\") as fh:\n",
        "        for ln in fh:\n",
        "            if not ln.strip() or ln.startswith(\"track\") or ln.startswith(\"#\"):\n",
        "                continue\n",
        "            parts = re.split(r\"\\s+|\\t+\", ln.strip())\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "            chrom = canonical(parts[0])\n",
        "            start0 = int(parts[1])  # BED start (0-based)\n",
        "            end1   = int(parts[2])  # BED end   (1-based)\n",
        "            name   = parts[3] if len(parts) >= 4 else \".\"\n",
        "            score  = parts[4] if len(parts) >= 5 else \".\"\n",
        "            strand = parts[5] if len(parts) >= 6 else \".\"\n",
        "            # TSS position: use start for '+', end for '-' (BED heuristic)\n",
        "            pos1 = start0 + 1 if strand == \"+\" else end1\n",
        "            rows.append((chrom, pos1, name, strand))\n",
        "    df = pd.DataFrame(rows, columns=[\"chrom\",\"pos1\",\"name\",\"strand\"])\n",
        "    return df\n",
        "\n",
        "def build_tss_index(df):\n",
        "    \"\"\"Index by chromosome: sorted arrays of positions and names.\"\"\"\n",
        "    idx = {}\n",
        "    for can, sub in df.groupby(\"chrom\"):\n",
        "        o = sub.sort_values(\"pos1\")\n",
        "        idx[can] = (o[\"pos1\"].to_numpy(), o[\"name\"].astype(str).to_numpy(), o[\"strand\"].astype(str).to_numpy())\n",
        "    return idx\n",
        "\n",
        "def nearest_pos_name(arr_pos, arr_name, q):\n",
        "    \"\"\"Return (pos, name, dist) nearest to query q in arr_pos.\"\"\"\n",
        "    import bisect\n",
        "    i = bisect.bisect_left(arr_pos, q)\n",
        "    cand = []\n",
        "    if i < len(arr_pos): cand.append((abs(arr_pos[i]-q), arr_pos[i], arr_name[i]))\n",
        "    if i > 0:            cand.append((abs(arr_pos[i-1]-q), arr_pos[i-1], arr_name[i-1]))\n",
        "    if not cand: return (None, None, None)\n",
        "    d, p, nm = min(cand, key=lambda z: z[0])\n",
        "    return (int(p), str(nm), int(d))\n",
        "\n",
        "# ------------- Peak detection -------------\n",
        "def detect_srnas_in_intergenics(bw_crp, bw_k52q, sizes_map, interg_by_chrom,\n",
        "                                smooth_win=31, min_width=50, max_width=350, min_prom=2e4,\n",
        "                                min_area=3e5, fold_min=5.0):\n",
        "    \"\"\"\n",
        "    For each intergenic region: use max(smooth(K52Q), smooth(CRP)) to locate peaks,\n",
        "    integrate area in CRP and K52Q over the same window, then apply filters.\n",
        "    Returns (ALL, HITS) as lists of dicts.\n",
        "    \"\"\"\n",
        "    ALL = []\n",
        "    HITS = []\n",
        "    for can, intervals in interg_by_chrom.items():\n",
        "        real = sizes_map[can][0]\n",
        "        for s,e in intervals:\n",
        "            L = e - s + 1\n",
        "            if L < min_width:\n",
        "                continue\n",
        "            # extract\n",
        "            crp = values_from_bw(bw_crp, real, s, e)\n",
        "            k52 = values_from_bw(bw_k52q, real, s, e)\n",
        "            # smoothing\n",
        "            crp_s = moving_average(crp, smooth_win)\n",
        "            k52_s = moving_average(k52, smooth_win)\n",
        "            # combined track\n",
        "            comb = np.maximum(crp_s, k52_s)\n",
        "            # find_peaks: widths in number of samples (1 sample = 1 bp)\n",
        "            peaks, props = find_peaks(comb, prominence=min_prom, width=(min_width, max_width))\n",
        "            if peaks.size == 0:\n",
        "                continue\n",
        "            # convert widths to integer bounds\n",
        "            left_ips  = props.get(\"left_ips\", np.array([], dtype=float))\n",
        "            right_ips = props.get(\"right_ips\", np.array([], dtype=float))\n",
        "            prominences = props.get(\"prominences\", np.zeros_like(peaks, dtype=float))\n",
        "            for i, pk in enumerate(peaks):\n",
        "                ls = int(max(0, math.floor(left_ips[i]))) if i < len(left_ips) else max(0, pk - min_width//2)\n",
        "                rs = int(min(L-1, math.ceil(right_ips[i]))) if i < len(right_ips) else min(L-1, pk + min_width//2)\n",
        "                # area (raw sum, not smoothed)\n",
        "                area_crp = float(np.sum(crp[ls:rs+1]))\n",
        "                area_k52 = float(np.sum(k52[ls:rs+1]))\n",
        "                width_bp = int(rs - ls + 1)\n",
        "                prom     = float(prominences[i]) if i < len(prominences) else float(comb[pk])\n",
        "                # collect ALL\n",
        "                rec = {\n",
        "                    \"chrom_can\": can,\n",
        "                    \"start1\":    int(s + ls),\n",
        "                    \"end1\":      int(s + rs),\n",
        "                    \"peak_pos1\": int(s + pk),\n",
        "                    \"width_bp\":  width_bp,\n",
        "                    \"prominence\": prom,\n",
        "                    \"area_CRP\":  area_crp,\n",
        "                    \"area_K52Q\": area_k52,\n",
        "                }\n",
        "                ALL.append(rec)\n",
        "                # filters (EXAMPLE thresholds)\n",
        "                mag = max(area_crp, area_k52)\n",
        "                sm  = min(area_crp, area_k52) + 1e-9\n",
        "                fold = mag / sm\n",
        "                if (mag >= min_area) and (fold >= fold_min):\n",
        "                    who = \"K52Q>CRP\" if area_k52 >= area_crp else \"CRP>K52Q\"\n",
        "                    hit = rec | {\"fold\": float(fold), \"who\": who}\n",
        "                    HITS.append(hit)\n",
        "    return ALL, HITS\n",
        "\n",
        "def save_bed_and_tsv(outdir, label, strand, all_list, hits_list):\n",
        "    bed_all  = Path(outdir)/\"peaks\"/f\"{label}_ALL.{strand}.bed\"\n",
        "    bed_hits = Path(outdir)/\"peaks\"/f\"{label}_HITS.{strand}.bed\"\n",
        "    tsv      = Path(outdir)/\"peaks\"/f\"{label}_RESULTS.{strand}.tsv\"\n",
        "    # save BEDs\n",
        "    with open(bed_all, \"w\") as fa:\n",
        "        for r in all_list:\n",
        "            fa.write(f\"{r['chrom_can']}\\t{r['start1']-1}\\t{r['end1']}\\n\")\n",
        "    with open(bed_hits, \"w\") as fh:\n",
        "        for r in hits_list:\n",
        "            fh.write(f\"{r['chrom_can']}\\t{r['start1']-1}\\t{r['end1']}\\t{r.get('who','.')}\\t0\\t.\\n\")\n",
        "    # TSV\n",
        "    pd.DataFrame(hits_list).to_csv(tsv, sep=\"\\t\", index=False)\n",
        "    return bed_all, bed_hits, tsv\n",
        "\n",
        "# ------------- MAIN -------------\n",
        "def main():\n",
        "    print(\"== 2) QC: open BigWigs and determine common chromosomes ==\")\n",
        "    bws, commons, sizes_ref = qc_open_and_common_chroms(BIGWIGS)\n",
        "    if not commons:\n",
        "        raise RuntimeError(\"No common chromosomes among the BigWigs.\")\n",
        "\n",
        "    # 1) path checks\n",
        "    print(\"Checking inputs...\")\n",
        "    for cond in BIGWIGS:\n",
        "        for st in (\"+\",\"-\"):\n",
        "            p = BIGWIGS[cond][st]\n",
        "            print(os.path.exists(p), p)\n",
        "    for can, p in GFF3[\"GFF3\"].items():\n",
        "        print(os.path.exists(p), p)\n",
        "    print(os.path.exists(TSS_PLUS), TSS_PLUS)\n",
        "    print(os.path.exists(TSS_MINUS), TSS_MINUS)\n",
        "\n",
        "    print(\"\\n== 3) Healthy mask (CDS, rRNA, tRNA) and intergenics ==\")\n",
        "    covered_by_chrom, interg_by_chrom = read_mask_intergenic(GFF3[\"GFF3\"], commons, sizes_ref)\n",
        "    rep = report_mask_intergenic(covered_by_chrom, interg_by_chrom, sizes_ref)\n",
        "    rep_path = Path(OUT_DIR)/\"mask\"/\"mask_intergenic_report.tsv\"\n",
        "    rep.to_csv(rep_path, sep=\"\\t\", index=False)\n",
        "    print(rep)\n",
        "\n",
        "    # save BEDs\n",
        "    mask_bed = Path(OUT_DIR)/\"mask\"/\"mask_CDS_rRNA_tRNA.bed\"\n",
        "    inter_bed= Path(OUT_DIR)/\"mask\"/\"intergenic_from_CDS_rRNA_tRNA.bed\"\n",
        "    # remove if they already exist\n",
        "    for p in (mask_bed, inter_bed):\n",
        "        if p.exists(): p.unlink()\n",
        "    for can in sorted(covered_by_chrom.keys()):\n",
        "        write_bed(mask_bed, can, covered_by_chrom[can])\n",
        "        write_bed(inter_bed, can, interg_by_chrom[can])\n",
        "    print(\"BEDs saved:\", mask_bed, inter_bed)\n",
        "\n",
        "    print(\"\\n== 4) Signal profile (short smoothing) ==\")\n",
        "    prof = []\n",
        "    for st in (\"+\",\"-\"):\n",
        "        pr_crp  = profile_track_over_intergenics(bws[\"CRP\"][st], sizes_ref, interg_by_chrom, step=PROFILE_STEP, smooth_win=31)\n",
        "        pr_k52  = profile_track_over_intergenics(bws[\"K52Q\"][st], sizes_ref, interg_by_chrom, step=PROFILE_STEP, smooth_win=31)\n",
        "        pr_crp[\"track\"] = f\"CRP_{st}\"\n",
        "        pr_k52[\"track\"] = f\"K52Q_{st}\"\n",
        "        prof.extend([pr_crp, pr_k52])\n",
        "    df_prof = pd.DataFrame(prof)\n",
        "    prof_path = Path(OUT_DIR)/\"qc\"/\"profiles.tsv\"\n",
        "    df_prof.to_csv(prof_path, sep=\"\\t\", index=False)\n",
        "    print(df_prof)\n",
        "\n",
        "    print(\"\\n== 5) sRNA-like peak detection per strand (no fixed windows) ==\")\n",
        "    results = []\n",
        "    for st in (\"+\",\"-\"):\n",
        "        ALL, HITS = detect_srnas_in_intergenics(\n",
        "            bws[\"CRP\"][st], bws[\"K52Q\"][st], sizes_ref, interg_by_chrom,\n",
        "            smooth_win=SMOOTH_WIN, min_width=MIN_WIDTH_BP, max_width=MAX_WIDTH_BP,\n",
        "            min_prom=MIN_PROM, min_area=MIN_AREA, fold_min=FOLD_MIN,\n",
        "        )\n",
        "        bed_all, bed_hits, tsv = save_bed_and_tsv(OUT_DIR, \"SRNA\", st, ALL, HITS)\n",
        "        print(f\"[{st}] candidates (ALL): {len(ALL)}  |  HITS (filtered): {len(HITS)}\")\n",
        "        results.append((st, len(ALL), len(HITS), bed_all, bed_hits, tsv))\n",
        "\n",
        "    print(\"\\n== 6) Nearest TSS (strand-aware) ==\")\n",
        "    tss_plus = read_tss_file(TSS_PLUS)\n",
        "    tss_minus= read_tss_file(TSS_MINUS)\n",
        "    idx_plus = build_tss_index(tss_plus)\n",
        "    idx_minus= build_tss_index(tss_minus)\n",
        "\n",
        "    # attach nearest TSS to each RESULT.tsv\n",
        "    for st, n_all, n_hits, bed_all, bed_hits, tsv in results:\n",
        "        df = pd.read_csv(tsv, sep=\"\\t\")\n",
        "        if df.empty:\n",
        "            print(f\"[{st}] No HITS; skipping nearest TSS.\")\n",
        "            continue\n",
        "        pos_arr = []\n",
        "        name_arr= []\n",
        "        dist_arr= []\n",
        "        for _, r in df.iterrows():\n",
        "            can = r[\"chrom_can\"]\n",
        "            q   = int((r[\"start1\"] + r[\"end1\"])//2)  # peak center\n",
        "            if st == \"+\" and can in idx_plus:\n",
        "                pos, nm, dist = nearest_pos_name(idx_plus[can][0], idx_plus[can][1], q)\n",
        "            elif st == \"-\" and can in idx_minus:\n",
        "                pos, nm, dist = nearest_pos_name(idx_minus[can][0], idx_minus[can][1], q)\n",
        "            else:\n",
        "                pos, nm, dist = (None, None, None)\n",
        "            pos_arr.append(pos); name_arr.append(nm); dist_arr.append(dist)\n",
        "        df[\"nearest_tss_pos1\"] = pos_arr\n",
        "        df[\"nearest_tss_name\"] = name_arr\n",
        "        df[\"dist_tss_bp\"]      = dist_arr\n",
        "        out_tsv = Path(OUT_DIR)/\"peaks\"/f\"SRNA_RESULTS.{st}.nearest.tsv\"\n",
        "        df.to_csv(out_tsv, sep=\"\\t\", index=False)\n",
        "        print(f\"[{st}] RESULT + nearest TSS:\", out_tsv)\n",
        "\n",
        "    print(\"\\n== 8) Visualization ==\")\n",
        "    print(\"Use igv.org/app and load: BigWigs (+/−); intergenic and HITS BEDs; GFF3 (if available as .gff3).\")\n",
        "    print(\"To share from Drive, create read-only public links.\")\n",
        "\n",
        "    print(\"\\n== 9) Quick diagnostics ==\")\n",
        "    rep = pd.read_csv(Path(OUT_DIR)/\"mask\"/\"mask_intergenic_report.tsv\", sep=\"\\t\")\n",
        "    if (rep[\"n_intergenic\"]==0).any():\n",
        "        print(\"intergenics = 0 → GFF3 and BigWigs do not match (names/versions). Check '.1' and chromosome names.\")\n",
        "    # check HITS\n",
        "    totals = sum(x[2] for x in results)\n",
        "    alls   = sum(x[1] for x in results)\n",
        "    if alls == 0:\n",
        "        print(\"ALL=0 → thresholds too high (MIN_PROM/MIN_WIDTH/SMOOTH_WIN) or incompatible signal.\")\n",
        "    elif totals == 0:\n",
        "        print(\"HITS=0 → FOLD_MIN and/or MIN_AREA too high. Adjust using p95/p99 from the profile.\")\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "# Run\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKcnWoK9KwRZ",
        "outputId": "c7458997-5e55-4e0a-f430-219f74a228a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== 2) QC: open BigWigs and determine common chromosomes ==\n",
            "[QC] Common chromosomes (canonical): ['NC_002505', 'NC_002506']\n",
            "Checking inputs...\n",
            "True /content/drive/MyDrive/colab_data/20250525_RNAseq_traces_stranded/CRP_A_plus_stranded.bw\n",
            "True /content/drive/MyDrive/colab_data/20250525_RNAseq_traces_stranded/CRP_A_minus_stranded.bw\n",
            "True /content/drive/MyDrive/colab_data/20250525_RNAseq_traces_stranded/K52Q_A_plus_stranded.bw\n",
            "True /content/drive/MyDrive/colab_data/20250525_RNAseq_traces_stranded/K52Q_A_minus_stranded.bw\n",
            "True /content/drive/MyDrive/colab_data/20250821_chip_analysis/data/raw/20250220_NC002505_GFF3converted.xlsx\n",
            "True /content/drive/MyDrive/colab_data/20250821_chip_analysis/data/raw/20250220_NC002506_GFF3converted.xlsx\n",
            "True /content/drive/MyDrive/colab_data/20250821_chip_analysis/data/raw/250221_TSS_plus.txt\n",
            "True /content/drive/MyDrive/colab_data/20250821_chip_analysis/data/raw/250221_TSS_minus.txt\n",
            "\n",
            "== 3) Healthy mask (CDS, rRNA, tRNA) and intergenics ==\n",
            "       chrom   len_bp  masked_bp  intergenic_bp   masked_%  intergenic_%  \\\n",
            "0  NC_002505  2961149    2635567         325582  89.004876     10.995124   \n",
            "1  NC_002506  1072315     907411         164904  84.621683     15.378317   \n",
            "\n",
            "   n_intergenic  \n",
            "0          2323  \n",
            "1           907  \n",
            "BEDs saved: /content/drive/MyDrive/colab_data/20251007_traces_scan/output/srna_scan_251008/mask/mask_CDS_rRNA_tRNA.bed /content/drive/MyDrive/colab_data/20251007_traces_scan/output/srna_scan_251008/mask/intergenic_from_CDS_rRNA_tRNA.bed\n",
            "\n",
            "== 4) Signal profile (short smoothing) ==\n",
            "   count_points      median         MAD           p95            p99  \\\n",
            "0        103292  964.142903  896.313904  30174.248614  146595.111754   \n",
            "1        103292  990.341927  915.316119  28244.741935  147638.870536   \n",
            "2        103292  647.467104  605.695007  23977.193548  131736.709677   \n",
            "3        103292  754.587119  695.788153  25758.833125  160379.500302   \n",
            "\n",
            "   frac_gt0   track  \n",
            "0  0.972999   CRP_+  \n",
            "1  0.973047  K52Q_+  \n",
            "2  0.948079   CRP_-  \n",
            "3  0.955573  K52Q_-  \n",
            "\n",
            "== 5) sRNA-like peak detection per strand (no fixed windows) ==\n",
            "[+] candidates (ALL): 255  |  HITS (filtered): 20\n",
            "[-] candidates (ALL): 172  |  HITS (filtered): 14\n",
            "\n",
            "== 6) Nearest TSS (strand-aware) ==\n",
            "[+] RESULT + nearest TSS: /content/drive/MyDrive/colab_data/20251007_traces_scan/output/srna_scan_251008/peaks/SRNA_RESULTS.+.nearest.tsv\n",
            "[-] RESULT + nearest TSS: /content/drive/MyDrive/colab_data/20251007_traces_scan/output/srna_scan_251008/peaks/SRNA_RESULTS.-.nearest.tsv\n",
            "\n",
            "== 8) Visualization ==\n",
            "Use igv.org/app and load: BigWigs (+/−); intergenic and HITS BEDs; GFF3 (if available as .gff3).\n",
            "To share from Drive, create read-only public links.\n",
            "\n",
            "== 9) Quick diagnostics ==\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}